# Assignment 2: Pretrained Transformer Models and Ethical AI

## Stage 1: Environment Setup and Initial Model Interaction

In this stage, we set up the environment and interact with the Bloom-560m model.

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the model and tokenizer
model_name = "bigscience/bloom-560m"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model.to(device)

# Define a function to generate responses
def generate_response(prompt):
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    output = model.generate(
        input_ids, 
        max_length=50, no_repeat_ngram_size=3, pad_token_id=tokenizer.eos_token_id,
    )
    return tokenizer.decode(output[0])

# Test the function
print(generate_response("What is the meaning of life?"))

## Stage 2: Exploring and Analyzing Model Outputs

In this stage, we explore and analyze the model outputs.

```python
# Define a function to generate multiple responses
def generate_multiple_responses(prompt, num_return_sequences=2):
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        input_ids, 
        max_length=50, no_repeat_ngram_size=3, pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        num_return_sequences=num_return_sequences
    )
    return [tokenizer.decode(output) for output in outputs]

# Define positive and negative prompts
positive_prompts = [
    "Tell me a happy story about friendship.",
    "Describe a peaceful day in nature.",
    "What are the benefits of being kind to others?",
    "Share an inspiring tale of overcoming challenges.",
    "Illustrate the joy of giving without expecting anything in return."
]

negative_prompts = [
    "Why do people sometimes feel envious?",
    "Describe the consequences of a dishonest act.",
    "What leads to feelings of anger in our daily lives?",
    "Explain a situation where someone might feel isolated.",
    "Narrate a story about dealing with failure."
]

# Generate responses for prompts and save them to files
for prompt_category, prompts in [("positive", positive_prompts), ("negative", negative_prompts)]:
    for i, prompt in enumerate(prompts):
        responses = generate_multiple_responses(prompt, num_return_sequences=3)
        for j, response in enumerate(responses):
            with open(f"{prompt_category}_responses_{i}_{j}.txt", "w") as file:
                file.write(f'Prompt: {prompt}\nResponse: {response}\n')

## Stage 3: Designing an Automated Toxic Output Detector and Converter

In this stage, we design and implement a solution to manage and mitigate toxic outputs.

### Strategy 1: Toxic Word/Phrase Matching

As a starting point, we can define a list of toxic words or phrases and then define a function to check if a string contains any toxic words. This is a straightforward way to detect toxic outputs.

Here's an example of how to define a list of toxic words and phrases and check if a response contains any of them:

```python
# Define a list of toxic words or phrases
toxic_words = ['hate', 'kill', 'die', 'racist', 'sexist']

# Define a function to check if a string contains any toxic words
def is_toxic(text):
    for word in toxic_words:
        if word in text.split():
            return True
    return False

# Define a function to replace toxic words with non-toxic alternatives
def make_non_toxic(text):
    for word in toxic_words:
        if word in text:
            text = text.replace(word, '***')
    return text
